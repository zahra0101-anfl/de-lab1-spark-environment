# ================= Docker Compose: Spark Master + Workers =================
# Create external network (run once):
#   docker network create sparknet
#
# Start cluster:
#   docker compose up -d
#
# Scale workers (example: 3 workers):
#   docker compose up -d --scale spark-worker=3

version: "3.9"

networks:
  sparknet:
    external: true

volumes:
  spark_data:

services:
  # ==================== Spark Master ====================
  spark-master:
    image: apache/spark:${SPARK_VERSION:-latest}
    container_name: spark-master
    restart: unless-stopped
    command: >
      bash -c "/opt/spark/sbin/start-master.sh && tail -f /dev/null"
    ports:
      - "${SPARK_MASTER_WEBUI_PORT:-8080}:${SPARK_MASTER_WEBUI_PORT:-8080}"
      - "${SPARK_MASTER_PORT:-7077}:${SPARK_MASTER_PORT:-7077}"
    environment:
      SPARK_MASTER_HOST: 0.0.0.0
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT:-7077}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT:-8080}
      SPARK_PUBLIC_DNS: "${SPARK_PUBLIC_DNS:-localhost}"
    volumes:
      - spark_data:/opt/spark/work-dir
      # bind mount for labs, you will need it later hint : it s a simlink 
      # - ./spark-data:/opt/spark/data  
    networks:
      - sparknet
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${SPARK_MASTER_WEBUI_PORT:-8080}/"]
      interval: 15s
      timeout: 5s
      retries: 5

  # ==================== Spark Worker(s) ====================
  spark-worker:
    image: apache/spark:${SPARK_VERSION:-latest}
    # NOTE: no container_name here to allow scaling
    restart: unless-stopped
    depends_on:
      spark-master:
        condition: service_healthy
    command: >
      bash -c "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /dev/null"
    ports:
      - "${SPARK_WORKER_WEBUI_PORT:-8081}:${SPARK_WORKER_WEBUI_PORT:-8081}"
    environment:
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES:-2}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY:-2g}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT:-8081}
      SPARK_PUBLIC_DNS: "${SPARK_PUBLIC_DNS:-localhost}"
    volumes:
      - spark_data:/opt/spark/work-dir
      # - ./spark-data:/opt/spark/data 
    networks:
      - sparknet